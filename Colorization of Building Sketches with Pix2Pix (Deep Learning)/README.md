# Colorization of Building Sketches with Pix2Pix (Deep Learning)

This project aims to transform binary input images into corresponding colorized images.

## Project Description

The Pix2Pix network architecture leverages the conditional GAN (Generative Adversarial Network) structure to generate and transform data. In our case, the input images are binary sketches, and we expect the network to produce colorized versions that match the original colors present in the dataset.

### Generator Structure

The generator in Pix2Pix follows an "encoder-decoder" architecture, which is depicted below with dimension values at each layer. The input dimension is 3x256x256, and the network aims to return data of the same initial shape as output. The generator takes input, processes it through encoders (comprising convolution and activation functions), and then employs decoders to return the transformed output.

Note: To enhance network performance and processing speed, an alternative is to use the U-Net network instead of the encoder-decoder. U-Net connects each encoder layer to its corresponding decoder layer in parallel, potentially eliminating the need for some layers.

### Descriptor

The role of the descriptor is to compare two images: one from the input and another from the output (target picture). It determines whether the second picture was generated by the generator or not.

The descriptor's structure resembles an encoder-decoder architecture but with distinctions. It produces a 30x30 binary image that assesses the similarity between different parts of the second image and the first image. In PIX2PIX implementation, each 30x30 pixel block is mapped to a 70x70 section of the input image.

To train the network effectively, both the discriminator (descriptor) and the generator need training. The discriminator evaluates and learns the naturalness of the generated images and influences the generator's weights based on its outputs.

### Challenges

Pix2Pix can produce remarkable results, but the labeling process and the requirement for paired input-output images (x and y) during training can be challenging. To address this, we explore cycleGANs, which build upon the Pix2Pix architecture and allow for the use of two separate sets of images.

If you would like more details, please take a look at the notebook file included in this project.

## Generated Images in Various Epochs

In the following sections, we showcase the progression of colorization in building sketches at different training epochs using the Pix2Pix model. These images illustrate the evolution of the model's performance over time.

### training
#### Epoch 5 to 10
![5-10](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/e54005f2-04ca-489e-9b94-50b4e58a38ef)


#### Epoch 20
![Uploading 20.PNGâ€¦]()

#### Epoch 30
![30](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/8b5a32bf-a14e-4ecf-bfc9-488c972a7445)


#### Epoch 40
![40](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/89e63f60-517a-4460-a06f-8574c8d361d6)


#### Epoch 49
![49](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/5c06b7d1-1f37-44c6-9b94-0a88642b1576)

### Test the network using the test dataset
![test1](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/0e786c32-2090-437b-a4ee-f52b288ce78a)

![test2](https://github.com/masoudrahimi39/Machine-Learning-Hands-On-Projects/assets/65596290/97097f79-6123-40a1-a8ed-f986b2141c43)
