{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# from sklearn.kernel_approximation import RBFSampler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## undrestanding the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "\n",
    "print('env action space:')\n",
    "print(f'\\t action space: \\t\\t\\t{env.action_space}')\n",
    "print(f'\\t a sample of action space: \\t{env.action_space.sample()}')\n",
    "print(f'\\t action space shape: \\t\\t{env.action_space.shape}')\n",
    "print(f'\\t action space lower bound: \\t{env.action_space.low}')\n",
    "print(f'\\t action space upper bound: \\t{env.action_space.high}')\n",
    "\n",
    "print('-----'*20)\n",
    "print('env observation space (state space):')\n",
    "print(f'\\t observation space: \\t\\t\\t{env.observation_space}')\n",
    "print(f'\\t a sample of observation space: \\t{env.observation_space.sample()}')\n",
    "print(f'\\t observation space shape: \\t\\t{env.observation_space.shape}')\n",
    "print(f'\\t observation space lower bound: \\t{env.observation_space.low}')\n",
    "print(f'\\t observation space upper bound: \\t{env.observation_space.high}')\n",
    "\n",
    "print('-----'*20)\n",
    "print(''' env reward:   \\t\\tA negative reward of -0.1 * action2 is received at each timestep to penalise for taking actions of large \n",
    "                        magnitude. If the mountain car reaches the goal then a positive reward of +100 is added to the negative \n",
    "                        reward for that timestep.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2c from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, I borrowed a little https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_state_coder(*, s, rbf_c, rbf_cov):\n",
    "    ''' based on algorithm in a book named: \"Reinforcement Learning: An Introduction second edition. Richard S. Sutton and Andrew G. Barto.\"\n",
    "        used to code the state by RBF method. ach RBF has a center(=rbf_c[0, :]) and a covariance(=rbf_cov[0, :]).\n",
    "        ** d : number of the rbfs in the state space\n",
    "\n",
    "        input\n",
    "        --------\n",
    "            s : state : shape (2, 1) : each shape has a value in range of [0, 1]\n",
    "\n",
    "            rbf_c : a np array : shape (d, 2) : center of each rbf is stored in each row\n",
    "                rbf_c_0 : rbf_c[0, :] : 1st row of rbf_c : determines the center of the 1st rbf; \n",
    "                state space is 2D, so shape(rbf_c_0) =(1, 2)\n",
    "\n",
    "            rbf_cov : np array : shape (d, 2) : covariance (diagonal elements) of each rbf is stored in each row\n",
    "                rbf_cov[0, :] : first row of rbf_cov : detemines the diagonal element of the covariance amtrix in the 1st rbf.\n",
    "\n",
    "        output\n",
    "        --------\n",
    "            x_s : shape (d, 1) : coded value of the state : featurized representation of the space : on the Nili's notation it is Φ\n",
    "    '''\n",
    "    d = rbf_c.shape[0]                 # number of the rbf functions\n",
    "    x_s = np.empty((d, 1))             # define the x matrix\n",
    "    # calculate each row of the x based on the formula x_i(s) = exp(-(s - c_i)^T * Σ_i * (s - c_i))\n",
    "    for row in range(d):\n",
    "        x_s[row, 0] = np.exp(-0.5 * (s-rbf_c[row].reshape((2, 1))).T @ np.linalg.inv(np.diag(rbf_cov[row])) @ (s-rbf_c[row].reshape((2, 1))))[0, 0]\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"Value Function approximator. \"\"\"\n",
    "    def __init__(self, *, w=None, alpha_w=0.1, state_coding= None, rbf_c=None, rbf_cov=None):\n",
    "        '''\n",
    "            input\n",
    "            -------\n",
    "                w : 2D np array shape (..., 1); shape depends x_s : weight vector in value formula; value formula is v = w^T @ x_s ; \n",
    "                alpha_w : learning rate for w in update method\n",
    "                state_coding : the function which is used to code the state. state_coding ∈ {'rbf_state_coder', 'featurize_state'}\n",
    "                rbf_c : the centers in rbf state coding : shape(d, 2)\n",
    "                rbf_cov : the covariance in rbf state coding : shape (d, 2)\n",
    "        '''\n",
    "        if w == None:\n",
    "            self.w = np.zeros((rbf_c.shape[0], 1))            # weight vector. v_hat is a linear function of the weight vector\n",
    "        else:\n",
    "            self.w = w\n",
    "        self.alpha_w = alpha_w\n",
    "        self.rbf_c = rbf_c                      # rbf parameter for state coding\n",
    "        self.rbf_cov = rbf_cov                  # rbf parameter for state coding\n",
    "        self.x_s = None\n",
    "        self.state_coding = state_coding\n",
    "         \n",
    "    def predict(self, state):\n",
    "        '''\n",
    "            take the state and give the value of the state (v_hat) based on below formula\n",
    "                        v_hat(s, w) = w^T x_s\n",
    "            input\n",
    "            ------\n",
    "                state : shape (2, 1)\n",
    "            output\n",
    "            -------\n",
    "                v_hat : scalar\n",
    "        '''\n",
    "        # TODO : later change the state_coding input argumnet from string to function\n",
    "        if self.state_coding == 'rbf_state_coder':        # code the state using rbf_state_coder\n",
    "            x_s = rbf_state_coder(s=state, rbf_c=self.rbf_c, rbf_cov=self.rbf_cov)\n",
    "            \n",
    "        # calculate the v_hat by fomula -->>  v_hat(s, w) = w^T x_s ; then by .item() we change it to scalar \n",
    "        v_hat = (self.w.T @ x_s).item()   \n",
    "        return v_hat\n",
    "\n",
    "    def update(self, *, state, value_state, target):\n",
    "        '''\n",
    "            take state, value_state and target and update the weight vector using below formula\n",
    "                    w_new = w_old + alpha_w*(target-value_state)*x_s \n",
    "\n",
    "            input\n",
    "            ---------\n",
    "                state : np array : shape(2, 1) : \n",
    "                value_state : scalar \n",
    "                target : scalar ; target = reward + (discount_factor * value_next)\n",
    "        '''\n",
    "        # w_new = w_old + alpha_w*(target-value_state)*x_s\n",
    "        if self.state_coding == 'rbf_state_coder': \n",
    "            self.w = self.w + self.alpha_w*(target-value_state)*rbf_state_coder(s=state, rbf_c=self.rbf_c, rbf_cov=self.rbf_cov)  \n",
    "        elif self.state_coding == 'featurize_state':\n",
    "            self.w = self.w + self.alpha_w*(target-value_state)*featurize_state(state)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"The object contain alpha_theta (learning_rate) and policy parameter (theta_mu and theta_sigma). used to predict and upda\"\"\"\n",
    "    # TODO : later change the state_coding input argumnet from string to function\n",
    "    def __init__(self, theta_mu=None, theta_sigma=None, \n",
    "                 alpha_theta=0.001, state_coding= None, rbf_c=None, rbf_cov=None):\n",
    "        '''\n",
    "            input\n",
    "            ------- \n",
    "                theta_mu : the parametr of the mu function. the mu formula is mu = theta_mu.T @ x_mu_s : shape(d, 1)\n",
    "                theta_sigma : the parametr of the sigma function. the sigma formula is sigma = exp(theta_sigma.T @ x_sigma_s)\n",
    "                alpha_theta : scalar : learning rate in learning of the theta (theta is the policy parameter)\n",
    "                state_coding : the function which is used to code the state. state_coding ∈ {'rbf_state_coder', 'featurize_state'}\n",
    "                rbf_c, rbf_cov : if state_coding=='rbf_state_coder' these arguments are used in state coding function.\n",
    "        '''\n",
    "        if (theta_mu == None) and (theta_sigma == None):\n",
    "            self.theta_mu = np.zeros((rbf_c.shape[0], 1))                       # policy parameter for theta\n",
    "            self.theta_sigma = np.zeros((rbf_c.shape[0], 1))\n",
    "        else:\n",
    "            self.theta_mu = theta_mu                # policy parameter for theta\n",
    "            self.theta_sigma = theta_sigma\n",
    "        \n",
    "        self.alpha_theta = alpha_theta\n",
    "        self.state_coding = state_coding\n",
    "        self.rbf_c = rbf_c                      # rbf parameter for state coding\n",
    "        self.rbf_cov = rbf_cov                  # rbf parameter for state coding\n",
    "        self.x_s = None\n",
    "        \n",
    "    def predict(self, state):\n",
    "        '''\n",
    "            take the state and provide the action; \n",
    "            action is a smaple from a normal distribution with parametrized mu and sigma with respect to theta_mu and theta_sigma. \n",
    "            input\n",
    "            -----\n",
    "                state : shape (2, 1) \n",
    "            ouput\n",
    "            ------\n",
    "                action : shape (1,)\n",
    "            '''\n",
    "        # coding of the state for mu and sigma\n",
    "        if self.state_coding == 'rbf_state_coder':        # code the state using rbf_state_coder\n",
    "            x_mu_s = rbf_state_coder(s=state, rbf_c=self.rbf_c, rbf_cov=self.rbf_cov)\n",
    "            x_sigma_s = x_mu_s\n",
    "        elif self.state_coding == 'featurize_state':      # code the state using featurize_state\n",
    "            x_mu_s = featurize_state(state)\n",
    "            x_sigma_s = x_mu_s\n",
    "        else:\n",
    "            raise Exception(\"state_coding is not valid.\")\n",
    "            \n",
    "        # calculate the mu and the sigma in the state\n",
    "        self.mu = self.theta_mu.T @ x_mu_s\n",
    "        self.sigma = np.exp(self.theta_sigma.T @ x_sigma_s)     \n",
    "                # self.mu.shape = (1, 1) and self.sigma.shape = (1, 1)\n",
    "            \n",
    "        # take a sample from the normal dist; the sample is our action; the normal dist is the parametrized policy; \n",
    "        action = default_rng().normal(loc=self.mu[0], scale=self.sigma[0], size=1)\n",
    "        # clip the the in range [0, 1] and save it on the action\n",
    "        np.clip(action, a_min=0, a_max=1, out=action)\n",
    "        return action\n",
    "\n",
    "    def update(self, *, state, td_error, action):\n",
    "        '''\n",
    "            update policy parameters theta_mu and theta_sigma\n",
    "            Note: because in the actor critic function, update method is called after predict method on same state, \n",
    "                  I use mu and sigma  which have been calculated in the predict method.\n",
    "            \n",
    "            input\n",
    "            --------\n",
    "                state : np array : shape(2, 1) : \n",
    "                td_error : scalar ; td_error = reward + discount_factor * value_next - value_state\n",
    "                action : shape (1,) : action which has been taken\n",
    "        '''\n",
    "        # coding of the state for mu and sigma\n",
    "        if self.state_coding == 'rbf_state_coder':        # code the state using rbf_state_coder\n",
    "            x_mu_s = rbf_state_coder(s=state, rbf_c=self.rbf_c, rbf_cov=self.rbf_cov)\n",
    "            x_sigma_s = x_mu_s\n",
    "        elif self.state_coding == 'featurize_state':      # code the state using featurize_state\n",
    "            x_mu_s = featurize_state(state)\n",
    "            x_sigma_s = x_mu_s\n",
    "            \n",
    "        # update theta_mu and theta_sigma based on formula: θ = θ + α_θ * I * 𝛿 * ∇ln(π(A|S, θ)),  𝛿 = td_error\n",
    "        self.theta_mu = self.theta_mu + self.alpha_theta * td_error * (1/(self.sigma[0, 0])**2)*(action[0] - self.mu[0, 0]) * x_mu_s\n",
    "        self.theta_sigma = self.theta_sigma+self.alpha_theta*td_error*(((action[0] - self.mu[0, 0])**2/self.sigma[0, 0]**2) - 1) * x_sigma_s\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(*, env, estimator_policy, estimator_value, num_episodes, discount_factor=0.97):\n",
    "    \"\"\"\n",
    "        Actor Critic Algorithm. Optimizes the policy \n",
    "        function approximator using policy gradient.\n",
    "\n",
    "        input:\n",
    "        -------\n",
    "            env: OpenAI environment.\n",
    "            estimator_policy: an instance of PolicyEstimatory class\n",
    "            estimator_value: an instance of ValueEstimator calss\n",
    "            num_episodes: Number of episodes to run\n",
    "            discount_factor : \n",
    "\n",
    "        output:\n",
    "        -------\n",
    "            sum_reward_per_episode : np array : shape(num_episodes,) : each row contains sum of reward for corresponding episode\n",
    "    \"\"\"\n",
    "    sum_reward_per_episode = np.zeros(num_episodes)   \n",
    "    avg_reward_per_episode = np.zeros(num_episodes)\n",
    "    \n",
    "    # iterate over episodes\n",
    "    for i_episode in range(num_episodes):\n",
    "        info_per_episode = {'episod_number': i_episode, 'reward_episode': [], 'avg_reward': None}\n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():            \n",
    "            # Take a step\n",
    "            action = estimator_policy.predict(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "            # Update statistics            \n",
    "            sum_reward_per_episode[i_episode] += reward\n",
    "            info_per_episode['reward_episode'].append(reward)\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_state = estimator_value.predict(state)\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            \n",
    "            td_error = td_target - value_state\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state=state, value_state=value_state, target=td_target)\n",
    "            \n",
    "            # Update the policy estimator;   using the td error as our advantage estimate\n",
    "            estimator_policy.update(state=state, td_error=td_error, action=action)\n",
    "        \n",
    "            print(f\"\\rStep {t} @ Episode {i_episode+1}/{num_episodes} -- sum_Episode_reward ({sum_reward_per_episode[i_episode]:.2f})\", end='')\n",
    "            if done:\n",
    "                # update the avg_reward in info_per_episode\n",
    "                info_per_episode['avg_reward'] = sum(info_per_episode['reward_episode'])/len(info_per_episode['reward_episode'])\n",
    "                print(f'----> Episode {i_episode+1} -- avg reward {info_per_episode[\"avg_reward\"]:.2f}')\n",
    "                avg_reward_per_episode[i_episode] = info_per_episode['avg_reward']\n",
    "                # plot the episdoe details and save the picture\n",
    "                # plot_episode_info_2(info_per_episode)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "    return sum_reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 998 @ Episode 1/2 -- sum_Episode_reward (-24.89)Episode 1 --- avg reward -0.024916755542655097\n",
      "Step 998 @ Episode 2/2 -- sum_Episode_reward (-26.43)Episode 2 --- avg reward -0.026453537003343016\n"
     ]
    }
   ],
   "source": [
    "# the matrix which includes the rbf's centers. each row is one center\n",
    "rbf_c = np.array(list(itertools.product(\n",
    "    [-1.2, -1.1, -1, -0.9, -0.8, -0.75, -0.7, -0.65, 0.6, -0.55, -0.5, -0.45, -0.4, -0.35, -0.3, 0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6], \n",
    "    [-0.07, -0.0652, -0.0604, -0.0556, -0.0508, -0.046, -0.0412, -0.0364, -0.0316, -0.0268, -0.022, -0.0172, -0.0124, -0.0076, -0.0028, 0.002, 0.0068, 0.0116, 0.0164, 0.0212, 0.026, 0.0308, 0.0356, 0.0404, 0.0452, 0.05, 0.0548, 0.0596, 0.0644, 0.0692])))\n",
    "# the covariance matrix which includes the rbf's covariances. each row is the covariance for the coresponding center.\n",
    "rbf_cov = np.array([[0.06, 0.0001]]*rbf_c.shape[0])\n",
    "\n",
    "# used to turn of \n",
    "gym.logger.set_level(40)\n",
    "num_episodes = 2\n",
    "# make the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "policy_estimator = PolicyEstimator(state_coding='rbf_state_coder', rbf_c=rbf_c, rbf_cov=rbf_cov)\n",
    "value_estimator = ValueEstimator(state_coding='rbf_state_coder', rbf_c=rbf_c, rbf_cov=rbf_cov)\n",
    "sum_reward_per_episode = actor_critic(env=env, estimator_policy=policy_estimator, estimator_value=value_estimator, \n",
    "                                      num_episodes=num_episodes, discount_factor=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w and theta_mu and theta_sigma in a .npy file\n",
    "\n",
    "parameters = {'theta_mu': policy_estimator.theta_mu, 'theta_sigma': policy_estimator.theta_sigma,\n",
    "                'weight': value_estimator.w, 'sum_R_per_E': sum_reward_per_episode}\n",
    "\n",
    "if not os.path.exists('Actor-critic/value-policy-params/'):\n",
    "    os.makedirs('Actor-critic/value-policy-params/')\n",
    "\n",
    "cwd2 = os.getcwd()\n",
    "path2 = os.path.join(cwd2, 'Actor-critic/value-policy-params/')\n",
    "# save parameter in a .npy file \n",
    "np.save(path2+f'theta_&_w.npy', parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2c stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# make the A2C model\n",
    "a2c_model = A2C(\"MlpPolicy\", env, n_steps=50000, verbose=1)\n",
    "a2c_model.learn(total_timesteps=100000)\n",
    "a2c_model.save(\"a2c_mountain_car_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = a2c_model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "962e37265c34f5bd38e83a175ded23681bad7764e837e9570061bc9845aade56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
